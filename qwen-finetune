qwen-1.5B预训练模型：
wget --ftp-user=reina --ftp-password=sinianqwqqvq@123 ftp://sujinbo.bcc-gzbh.baidu.com//home/fdisk2/qwen2-1.5B.zip
微调代码：
wget --ftp-user=reina --ftp-password=sinianqwqqvq@123 ftp://sujinbo.bcc-gzbh.baidu.com//home/fdisk/llama-factory/llama_train.tar.gz
解压后cd进入主文件夹，运行如下指令安装依赖：
pip install -e ".[torch,metrics]"
几个比较重要的文件：
模型训练参数
examples/train_full/qwen_full_sft.yaml

### model
model_name_or_path: 

### method
stage: sft
do_train: true
finetuning_type: full
deepspeed: examples/deepspeed/ds_z3_config.json

### dataset
dataset: 
template: qwen
cutoff_len: 2048
max_samples: 400000000
overwrite_cache: true
preprocessing_num_workers: 64

### output
output_dir: 
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 64
gradient_accumulation_steps: 4
learning_rate: 1.0e-5
num_train_epochs: 10.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000


model_name_of_path:模型路径
stage:有pt和sft两种，数据格式不同，见下段数据形式
dataset：在dataset_info中设置的数据集名称
output_dir：输出路径
其他的参数含义同参数名
deepspeed:ds设置，zero3我已经配好了，直接用即可，如果要使用zero-1/2的话，对应去examples/deepspeed/ds_z*_config.json里面配置即可。
数据集格式：
当有新数据集的时候，需要先处理成Alpaca format，处理脚本参考data/make_qwen_data.py
SFT数据集格式如下：
[
  {
    "instruction": "human instruction (required)",
    "input": "human input (optional)",
    "output": "model response (required)",
    "system": "system prompt (optional)",
    "history": [
      ["human instruction in the first round (optional)", "model response in the first round (optional)"],
      ["human instruction in the second round (optional)", "model response in the second round (optional)"]
    ]
  }
]
一个例子：
{
    "instruction": "Describe a process of making crepes.",
    "input": "",
    "output": "Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n\n1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n\n2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n\n3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n\n4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n\n5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n\n6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n\n7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n\n8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!"
}
pretrain数据集格式如下：
[
  {"text": "document"},
  {"text": "document"}
]
在处理完数据后，需要在dataset_info.json中加入对应的数据集信息，示例如下：
SFT:
"pertrain_data_for_ec_generate_for_qwen":{
    "file_name":"./dataset_name.jsonl",
    "columns": {
      "prompt": "instruction",
      "query": "input",
      "response": "output",
      "system": "system",
      "history": "history"
    }
  }
pretrain:
"dataset_name": {
  "file_name": "data.json",
  "columns": {
    "prompt": "text"
  }
}
上面的dataset参数填对应的数据集名即可
启动训练
单机多卡：
llamafactory-cli train examples/train_full/qwen_full_sft.yaml
多机多卡：
FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=0 MASTER_ADDR=192.168.0.1 MASTER_PORT=29500 llamafactory-cli train examples/train_full/qwen_full_sft.yaml
FORCE_TORCHRUN=1 NNODES=2 NODE_RANK=1 MASTER_ADDR=192.168.0.1 MASTER_PORT=29500 llamafactory-cli train examples/train_full/qwen_full_sft.yaml

批量推理
参考qwen_batch_infer.py
也可以自己用vllm/sglang启动一个服务做infer

可能会遇到的问题：
缺少flash_attn和flash_infer：
文件夹中有对应的包，根据py版本和torch版本选择，安装abiFALSE版本

deepspeed报错：
版本问题
pip install deepspeed==0.14.0
